{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naoya1110/road_following_by_classification/blob/main/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XDPz7h7mBs4"
      },
      "source": [
        "# Road Following by Classification - Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2QUA6Rawrek"
      },
      "source": [
        "## Introduction\n",
        "Once you have finished the data collection for you road following task, you need to train a model with your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r5NfcQO3Ypa"
      },
      "source": [
        "### GPU\n",
        "Please make sure we can use GPU for training a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0BLwskStrUB"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKQAYqgs2UI_"
      },
      "source": [
        "### General Python Packages\n",
        "Let's import general Python packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH57yMgb2VEh"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPpYl8py2CJz"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8PoOJaVmBs-"
      },
      "source": [
        "### Upload & Extract Dataset\n",
        "\n",
        "We need to upload the `dataset.zip` file in the file browser of the Google Colab. Uploading the zip file takes for a while depending the size of your dataset. After uploading is finished, you can extract it with the next command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIjqaJK6mBs_"
      },
      "source": [
        "! unzip -q dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4StH_vdmBs_"
      },
      "source": [
        "Now we can find a directory named ``dataset`` in the file browser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNe8qgKuxoQ7"
      },
      "source": [
        "#### *If your `dataset.zip` file is in your google drive*\n",
        "\n",
        "First you need to mount your google drive with the next command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g9GyxvLk_FY"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfGGKRB2xyV4"
      },
      "source": [
        "Then extract the `dataset.zip` file. Remenber to change the file path appropriate for your `dataset.zip`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_nrfm_PlNtO"
      },
      "source": [
        "!unzip -q /path/to/your/dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MN9UVYCz3nR"
      },
      "source": [
        "### Remove Broken Data\n",
        "The dataset directory may contain some broken image data or unnecessary directories for some reasons. Such data will cause errors when we train the model. So it is better to remove such data before training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piJmvrfI1Dgh"
      },
      "source": [
        "subdirs = sorted(os.listdir(\"dataset\"))\n",
        "\n",
        "for subdir in subdirs:\n",
        "    filenames = os.listdir(os.path.join(\"dataset\", subdir))\n",
        "    for filename in filenames:\n",
        "        path = os.path.join(\"dataset\", subdir, filename)\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            try:\n",
        "                os.remove(path)\n",
        "                print(\"Removed\", path)\n",
        "            except IsADirectoryError:\n",
        "                shutil.rmtree(path)\n",
        "                print(\"Removed\", path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EGR3ex31iEH"
      },
      "source": [
        "### Number of Image Data\n",
        "Let's check number of image data that we have now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwqyVR_6nDul"
      },
      "source": [
        "n_files = {}\n",
        "\n",
        "for subdir in subdirs:\n",
        "    n_file =  len(os.listdir(os.path.join(\"dataset\", subdir)))\n",
        "    n_files[subdir]=n_file\n",
        "\n",
        "print(n_files)\n",
        "plt.rcParams[\"font.size\"]=14\n",
        "plt.bar(n_files.keys(), n_files.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCwm8-zBmBtA"
      },
      "source": [
        "### Create Dataset\n",
        "Now we use the ``ImageFolder`` dataset class available with the ``torchvision.datasets`` package.  We attach transforms from the ``torchvision.transforms`` package to prepare the data for training.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqqeo3DpmBtB"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "means = [0.485, 0.456, 0.406]\n",
        "stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "dataset = datasets.ImageFolder(\n",
        "    'dataset',\n",
        "    transforms.Compose([\n",
        "        transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means, stds)\n",
        "    ])\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L54IEcBr23n7"
      },
      "source": [
        "### Class Index\n",
        "Let's check class index numbers that are automatically assigned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcU5OzE81j5q"
      },
      "source": [
        "dataset.class_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWbl2IJfM3ig"
      },
      "source": [
        "Then create classnames dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQn1DvoiMipw"
      },
      "source": [
        "classnames = {}\n",
        "\n",
        "for key, value in dataset.class_to_idx.items():\n",
        "    classnames[value] = key\n",
        "\n",
        "classnames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woi0SNXZmBtB"
      },
      "source": [
        "### Train Test Split\n",
        "We need to split the dataset into train and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyxBiWhVmBtC"
      },
      "source": [
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 100, 100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvgz5TAcmBtD"
      },
      "source": [
        "### Data Loaders\n",
        "Then we can make data loaders for train and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zDsK187mBtD"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=20,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=20,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF8ZUfqtzIri"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEenc3DFmBtE"
      },
      "source": [
        "### Model Architecture\n",
        "We build a relatively simple CNN model but you can use any models you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z6lv7cVtgJw"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64*7*7, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),    # dropout layer\n",
        "            nn.Linear(256, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)  \n",
        "        return x\n",
        "    \n",
        "model = Model().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiyURRAC3nt-"
      },
      "source": [
        "### torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhV2bhZztgHI"
      },
      "source": [
        "! pip install torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "for x_batch, _ in train_loader:\n",
        "    break\n",
        "\n",
        "input_shape = x_batch.shape\n",
        "print(input_shape)\n",
        "\n",
        "summary(model, input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqaeFfoOdJf7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYQgmwJPmBtF"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Now we can train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzQx5mSnpt3y"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = Model()\n",
        "model = model.to(device)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()                      # set loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=1E-4)    # set optimizer\n",
        "epochs = 20\n",
        "\n",
        "best_model_path = 'best_model.pth'\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# create empty lists for saving metrics during training\n",
        "train_loss_list = []\n",
        "train_accuracy_list = []\n",
        "test_loss_list = []\n",
        "test_accuracy_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"-----------------------------\")\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    # initialize metrics\n",
        "    train_correct_count = 0\n",
        "    train_accuracy = 0\n",
        "    train_loss = 0\n",
        "    test_correct_count = 0\n",
        "    test_accuracy = 0\n",
        "    test_loss = 0\n",
        "\n",
        "    #--- Training Phase ---#\n",
        "    model.train()    # set model to training mode\n",
        "\n",
        "    pbar = tqdm(train_loader)\n",
        "    pbar.set_description(\"Train\")\n",
        "\n",
        "    for x_batch, y_batch in pbar:      # take mini batch data from train_loader\n",
        "        \n",
        "        x_batch = x_batch.to(device)     # load x_batch data on GPU\n",
        "        y_batch = y_batch.to(device)     # load y_batch data on GPU\n",
        "    \n",
        "        optimizer.zero_grad()                  # reset gradients to 0\n",
        "        p_batch = model(x_batch)               # do prediction\n",
        "        loss = loss_func(p_batch, y_batch)     # measure loss\n",
        "        loss.backward()                        # calculate gradients\n",
        "        optimizer.step()                       # update model parameters\n",
        "\n",
        "        train_loss += loss.item()                                # accumulate loss value\n",
        "        p_batch_label = torch.argmax(p_batch, dim=1)             # convert p_batch vector to p_batch_label\n",
        "        train_correct_count += (p_batch_label == y_batch).sum()  # count up number of correct predictions\n",
        "\n",
        "        pbar.set_postfix({\"accuracy\":f\"{(p_batch_label == y_batch).sum()/len(x_batch):.4f}\", \"loss\": f\"{loss.item():.4f}\"})\n",
        "    #----------------------#\n",
        "\n",
        "    #--- Evaluation Phase ---#\n",
        "    with torch.no_grad():   # disable autograd for saving memory usage\n",
        "        model.eval()        # set model to evaluation mode\n",
        "\n",
        "        pbar = tqdm(test_loader)\n",
        "        pbar.set_description(\"Test\") \n",
        "\n",
        "        for x_batch, y_batch in pbar:   # take mini batch data from test_loader\n",
        "\n",
        "            x_batch = x_batch.to(device)     # load x_batch data on GPU\n",
        "            y_batch = y_batch.to(device)     # load y_batch data on GPU\n",
        "\n",
        "            p_batch = model(x_batch)              # do prediction\n",
        "            loss = loss_func(p_batch, y_batch)    # measure loss\n",
        "\n",
        "            test_loss += loss.item()                                # accumulate loss value\n",
        "            p_batch_label = torch.argmax(p_batch, dim=1)            # convert p_batch vector to p_batch_label\n",
        "            test_correct_count += (p_batch_label == y_batch).sum()  # count up number of correct predictions\n",
        "\n",
        "            pbar.set_postfix({\"accuracy\":f\"{(p_batch_label == y_batch).sum()/len(x_batch):.4f}\", \"loss\": f\"{loss.item():.4f}\"})\n",
        "    #------------------------#\n",
        "\n",
        "    train_accuracy = train_correct_count.item()/len(train_dataset)   # determine accuracy for training data\n",
        "    test_accuracy = test_correct_count.item()/len(test_dataset)      # determine accuracy for test data\n",
        "    train_loss = train_loss/len(train_loader)                        # determine loss for training data\n",
        "    test_loss = test_loss/len(test_loader)                           # determine loss for test data \n",
        "\n",
        "    # show and store metrics\n",
        "    print(f\"Train: Accuracy={train_accuracy:.3f} Loss={train_loss:.3f}, Test: Accuracy={test_accuracy:.3f} Loss={test_loss:.3f}\")\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    train_loss_list.append(train_loss)\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "    test_loss_list.append(test_loss)\n",
        "\n",
        "    # save the model if test accuracy is better than before\n",
        "    if test_accuracy > best_accuracy:\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Test accuracy improved from {best_accuracy:.3f} to {test_accuracy:.3f}\")\n",
        "        print(f\"Model saved at {best_model_path}\")\n",
        "        best_accuracy = test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFRsHo9N34K7"
      },
      "source": [
        "### Learning Curves\n",
        "Let's check the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZlwSRJdtbyJ"
      },
      "source": [
        "plt.rcParams[\"font.size\"]=14\n",
        "real_epochs = np.arange(len(train_accuracy_list))+1\n",
        "\n",
        "plt.plot(real_epochs, train_accuracy_list, c=\"#ff7f0e\", label=\"train acc\")\n",
        "plt.plot(real_epochs, test_accuracy_list, lw=0, marker=\"o\", c=\"#ff7f0e\", label=\"test acc\")\n",
        "plt.plot(real_epochs, train_loss_list, c=\"#1f77b4\", label=\"train loss\")\n",
        "plt.plot(real_epochs, test_loss_list, lw=0, marker=\"o\", c=\"#1f77b4\", label=\"test loss\")\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy & Loss\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFYLCYrv4CB7"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbIQrLxheh5z"
      },
      "source": [
        "### Load The Best Model\n",
        "Let's load the best model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj8Ruep2tbjK"
      },
      "source": [
        "model.load_state_dict(torch.load(best_model_path))   # load model parameters to the initialized model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmeOCcqtelDy"
      },
      "source": [
        "### Test Accuracy\n",
        "Let's check the accuracy for the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCE_DFhxtbcR"
      },
      "source": [
        "test_accuracy = 0\n",
        "\n",
        "y_test_all = np.array([])\n",
        "p_label_all = np.array([])\n",
        "\n",
        "with torch.no_grad():    # disable autograd\n",
        "    model.eval()         # set model to evaluation mode\n",
        "    \n",
        "    for x_batch, y_batch in test_loader:    # take mini batch data from train_loader\n",
        "        x_batch = x_batch.to(device)        # transfer x_batch to gpu\n",
        "        y_batch = y_batch.to(device)        # transfer y_batch to gpu\n",
        "        p_batch = model(x_batch)            # do prediction\n",
        "\n",
        "        p_batch_label = torch.argmax(p_batch, dim=1)       # convert p_batch vector to p_batch_label\n",
        "        test_accuracy += (p_batch_label == y_batch).sum()  # count up number of correct predictions\n",
        "\n",
        "        y_test_all = np.append(y_test_all, y_batch.to(\"cpu\").numpy())          # append y_batch in y_test_all\n",
        "        p_label_all = np.append(p_label_all, p_batch_label.to(\"cpu\").numpy())  # append p_batch_label in p_label_all\n",
        "\n",
        "test_accuracy = test_accuracy/len(test_dataset)      # determine accuracy for test data\n",
        "print(f\"Test Accuracy = {test_accuracy:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MKz-8gsev5r"
      },
      "source": [
        "### Confusion Matrix\n",
        "Let's see the confusion matrix for the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vzxYjQByszv"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cmx = confusion_matrix(y_test_all, p_label_all)\n",
        "\n",
        "cmx_pct = np.zeros(cmx.shape)\n",
        "\n",
        "for i in range(cmx.shape[0]):\n",
        "    for j in range(cmx.shape[1]):\n",
        "        cmx_pct[i, j] = cmx[i, j]/cmx[i, :].sum()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "labels = classnames.values()\n",
        "\n",
        "sns.heatmap(cmx_pct, annot=True, fmt=\".2f\", cmap=\"Blues\", vmin=0, vmax=1,\n",
        "            xticklabels=classnames.values(), yticklabels=classnames.values(), square=True)\n",
        "\n",
        "plt.ylabel(\"True\")\n",
        "plt.xlabel(\"Pred\")\n",
        "plt.title(\"confusion matrix\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSmr3eXie0hq"
      },
      "source": [
        "### Predictions for Test Data\n",
        "Let's see the predictions for the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7jCGoOKywy6"
      },
      "source": [
        "plt.figure(figsize=(20, 14))\n",
        "\n",
        "for i in range(50):\n",
        "    image, _ = test_dataset[i]\n",
        "    image = np.transpose(image, (1,2,0))\n",
        "\n",
        "    image[:, :, 0] = image[:, :, 0]*stds[0]+means[0]\n",
        "    image[:, :, 1] = image[:, :, 1]*stds[1]+means[1]\n",
        "    image[:, :, 2] = image[:, :, 2]*stds[2]+means[2]\n",
        "\n",
        "    plt.subplot(5, 10, i+1)\n",
        "    plt.imshow(image)\n",
        "\n",
        "    true_class = classnames[y_test_all[i]]\n",
        "    pred_class = classnames[p_label_all[i]]\n",
        "    if true_class == pred_class:\n",
        "        color = \"green\"\n",
        "    else:\n",
        "        color = \"red\"\n",
        "    plt.title(f\"T={true_class}\\nP={pred_class}\", color=color)\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVJk_B4GmBtG"
      },
      "source": [
        "## Download Model\n",
        "Once you finished training, download `best_model.pth` from the file browser and upload it to your JetBot."
      ]
    }
  ]
}