{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKwvuMxsFghR"
   },
   "source": [
    "# Road Following by Classification - Live Demo\n",
    "\n",
    "In this notebook, we will try to make the JetBot to follow the desired road by using the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9Ae369MGcZP"
   },
   "source": [
    "## Model Architecture\n",
    "First we define a DNN model. This needs to be identical to what used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wJFwgmW3FghW",
    "outputId": "2c810d06-2a6e-4f4a-d935-3e23529b2d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda')\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),    # dropout layer\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)  \n",
    "        return x\n",
    "    \n",
    "model = Model().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjJCDzvGFghX"
   },
   "source": [
    "## Load The Best Model\n",
    "Next we need to upload the `best_model.pth` in the file browser.\n",
    "\n",
    "Then load the parameters on the model from the `best_model.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3kYGMn_ZFghX",
    "outputId": "7370ecc2-c793-4ea5-9ad8-33e69798ac44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRGQRpcBFghY"
   },
   "source": [
    "## Preprocessing Function\n",
    "\n",
    "Now we create a function for preprocessing image data taken by the camera. This is very similar to what we have done in the collision avoidance example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eTaXtdgTFghY"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    # x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k2V5NKkHkAm"
   },
   "source": [
    "## Camera Instance\n",
    "Now we create a camera instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EdgXKPloFgha"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556dfce64df940f6b56c0a1d04cd0e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import traitlets\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "image = widgets.Image(format='jpeg', width=300, height=300)\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Let's try to make an inference. This process takes for a while for the first time because it needs to load a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    x = camera.value\n",
    "    x = preprocess(x)\n",
    "    y = model(x)\n",
    "    y = F.softmax(y, dim=1)\n",
    "    y_idx = torch.argmax(y, dim=1).item()\n",
    "    print(y_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StFLmt8uFghc"
   },
   "source": [
    "## Robot Instance\n",
    "Create the robot instance to drive the motors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bCsq9LspFghc"
   },
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNsLPGsMFghc"
   },
   "source": [
    "## Define Actions\n",
    "Create functions of move forward, turn left and turn right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "b8yMHZ0VFghc"
   },
   "outputs": [],
   "source": [
    "def move_forward():\n",
    "    robot.set_motors(0.2, 0.2)\n",
    "\n",
    "def turn_left():\n",
    "    robot.set_motors(0.0, 0.2)\n",
    "\n",
    "def turn_right():\n",
    "    robot.set_motors(0.2, 0.0)\n",
    "    \n",
    "actions_dict = {0:\"Go Forward\", 1:\"Turn Left\", 2:\"Turn Right\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uILzSjgmKEXg"
   },
   "source": [
    "## Run JetBot\n",
    "Run JetBot with a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9kLBthwAFghc",
    "outputId": "863df5c6-2f49-42ab-e48a-bcba8f0ba838"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556dfce64df940f6b56c0a1d04cd0e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:1000/1000   Action:Turn Left   FPS:31.3"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "display(image)\n",
    "steps = 1000\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(steps):\n",
    "        x = camera.value\n",
    "        x = preprocess(x)\n",
    "        y = model(x)\n",
    "        y = nn.Softmax(dim=1)(y)\n",
    "        y_idx = torch.argmax(y, dim=1).item()\n",
    "\n",
    "        if y_idx == 0: move_forward()\n",
    "        elif y_idx == 1: turn_left()\n",
    "        elif y_idx == 2: turn_right()\n",
    "        \n",
    "        now = time.time()\n",
    "        dt = now-t0\n",
    "        t0 = now\n",
    "        FPS = 1/dt\n",
    "        \n",
    "        print(f\"\\rStep:{i+1}/{steps}   Action:{actions_dict[y_idx]}   FPS:{FPS:.1f}\", end=\"\")\n",
    "\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHUWbWd3I6c2"
   },
   "source": [
    "If you are done, stop the robot and the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6HPA2wWFghc"
   },
   "outputs": [],
   "source": [
    "robot.stop()\n",
    "camera.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "live_demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
